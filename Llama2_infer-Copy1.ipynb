{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjyPTF-Qa8zk",
    "outputId": "a046b5a2-23ec-41e1-c9cd-0aad41356817",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "id": "4y3SiZdBbcMP",
    "outputId": "5a5721e7-1fde-4d3a-8489-8ccf10fce982",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-19 08:34:37,913] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bf944fd87f438a841c8eea3b111c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:8091\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:8091/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "from model import run\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an expert in movie review analysis and give the score out of 10 without any other explanation with these corresponding category. If some of the aspects is not clearly mentioned, leave it blank.\n",
    "- Plot (Story Arc and Plausibility):\n",
    "- Attraction (Premise & Entertainment Value):\n",
    "- Theme (Identity & Depth):\n",
    "- Acting (Characters & Performance):\n",
    "- Dialogue (Storytelling & Context):\n",
    "- Cinematography (Visual Language & Lighting, Setting, and Wardrobe):\n",
    "- Editing (Pace & Effects):\n",
    "- Soundtrack (Sound Design & Film Score):\n",
    "- Directing (Vision & Execution): \n",
    "Overall score:\n",
    "Just give the score, no other explanation. If some of the aspects is not clearly mentioned, leave it blank.\n",
    "\n",
    "Q: This movie was bad, really bad. It is longer than it should have been. It has no twists and no interesting story. \n",
    "- Plot: 1\n",
    "- Attraction: 4\n",
    "- Theme:\n",
    "- Acting:\n",
    "- Dialogue: 2\n",
    "- Cinematography:\n",
    "- Editing:\n",
    "- Soundtrack:\n",
    "- Directing: 2\n",
    "Overall score: 2\n",
    "\"\"\"\n",
    "\n",
    "MAX_MAX_NEW_TOKENS = 2048\n",
    "DEFAULT_MAX_NEW_TOKENS = 500\n",
    "\n",
    "DESCRIPTION = \"\"\"\n",
    "# Llama-2 7B Chat for Movie Review Analysis\n",
    "\"\"\"\n",
    "\n",
    "LICENSE = \"\"\"\n",
    "<p>\n",
    "As a derivate work of Llama-2-7b-chat by Meta, this model is governed by the original license.\n",
    "</p>\n",
    "\"\"\"\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    DESCRIPTION += '\\n<p>Running on CPU ü•∂ This demo does not work on CPU.</p>'\n",
    "\n",
    "\n",
    "def clear_and_save_textbox(message: str) -> tuple[str, str]:\n",
    "    return '', message\n",
    "\n",
    "\n",
    "def display_input(message: str,\n",
    "                  history: list[tuple[str, str]]) -> list[tuple[str, str]]:\n",
    "    history.append((message, ''))\n",
    "    return history\n",
    "\n",
    "\n",
    "def delete_prev_fn(\n",
    "        history: list[tuple[str, str]]) -> tuple[list[tuple[str, str]], str]:\n",
    "    try:\n",
    "        message, _ = history.pop()\n",
    "    except IndexError:\n",
    "        message = ''\n",
    "    return history, message or ''\n",
    "\n",
    "\n",
    "def generate(\n",
    "    message: str,\n",
    "    history_with_input: list[tuple[str, str]],\n",
    "    max_new_tokens: int,\n",
    "    top_p: float,\n",
    "    temperature: float,\n",
    "    top_k: int,\n",
    "    system_prompt = DEFAULT_SYSTEM_PROMPT\n",
    ") -> Iterator[list[tuple[str, str]]]:\n",
    "    if max_new_tokens > MAX_MAX_NEW_TOKENS:\n",
    "        raise ValueError\n",
    "\n",
    "    history = history_with_input[:-1]\n",
    "    generator = run(message, history, system_prompt, max_new_tokens,\n",
    "                    temperature, top_p, top_k)\n",
    "    try:\n",
    "        first_response = next(generator)\n",
    "        yield history + [(message, first_response)]\n",
    "    except StopIteration:\n",
    "        yield history + [(message, '')]\n",
    "    for response in generator:\n",
    "        yield history + [(message, response)]\n",
    "\n",
    "\n",
    "def process_example(message: str) -> tuple[str, list[tuple[str, str]]]:\n",
    "    generator = generate(message, [], 1024, 0.95, 1,\n",
    "                         1000)\n",
    "    for x in generator:\n",
    "        pass\n",
    "    return '', x\n",
    "\n",
    "\n",
    "with gr.Blocks(css='style.css') as demo:\n",
    "    gr.Markdown(DESCRIPTION)\n",
    "\n",
    "    with gr.Group():\n",
    "        chatbot = gr.Chatbot(label='Chatbot')\n",
    "        with gr.Row():\n",
    "            textbox = gr.Textbox(\n",
    "                container=False,\n",
    "                show_label=False,\n",
    "                placeholder='Type a message...',\n",
    "                scale=10,\n",
    "            )\n",
    "            submit_button = gr.Button('Submit',\n",
    "                                      variant='primary',\n",
    "                                      scale=1,\n",
    "                                      min_width=0)\n",
    "    with gr.Row():\n",
    "        retry_button = gr.Button('üîÑ  Retry', variant='secondary')\n",
    "        undo_button = gr.Button('‚Ü©Ô∏è Undo', variant='secondary')\n",
    "        clear_button = gr.Button('üóëÔ∏è  Clear', variant='secondary')\n",
    "\n",
    "    saved_input = gr.State()\n",
    "\n",
    "    with gr.Accordion(label='Advanced options', open=False):\n",
    "        max_new_tokens = gr.Slider(\n",
    "            label='Max new tokens',\n",
    "            minimum=1,\n",
    "            maximum=MAX_MAX_NEW_TOKENS,\n",
    "            step=1,\n",
    "            value=DEFAULT_MAX_NEW_TOKENS,\n",
    "        )\n",
    "        temperature = gr.Slider(\n",
    "            label='Temperature',\n",
    "            minimum=0.1,\n",
    "            maximum=4.0,\n",
    "            step=0.1,\n",
    "            value=1.0,\n",
    "        )\n",
    "        top_p = gr.Slider(\n",
    "            label='Top-p (nucleus sampling)',\n",
    "            minimum=0.05,\n",
    "            maximum=1.0,\n",
    "            step=0.05,\n",
    "            value=0.95,\n",
    "        )\n",
    "        top_k = gr.Slider(\n",
    "            label='Top-k',\n",
    "            minimum=1,\n",
    "            maximum=1000,\n",
    "            step=1,\n",
    "            value=50,\n",
    "        )\n",
    "\n",
    "    # gr.Examples(\n",
    "    #     examples=[\n",
    "    #         'Hello there! Who are you?',\n",
    "    #         'How to crawl data from an website using Python?',\n",
    "    #         'How to use some machine learning models in PySpark?',\n",
    "    #         'How do draw a graph in SparkML?',\n",
    "    #         \"How to create dataframe in python?\",\n",
    "    #     ],\n",
    "    #     inputs=textbox,\n",
    "    #     outputs=[textbox, chatbot],\n",
    "    #     fn=process_example,\n",
    "    #     cache_examples=True,\n",
    "    # )\n",
    "\n",
    "    gr.Markdown(LICENSE)\n",
    "\n",
    "    textbox.submit(\n",
    "        fn=clear_and_save_textbox,\n",
    "        inputs=textbox,\n",
    "        outputs=[textbox, saved_input],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=display_input,\n",
    "        inputs=[saved_input, chatbot],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            saved_input,\n",
    "            chatbot,\n",
    "            max_new_tokens,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "    )\n",
    "\n",
    "    button_event_preprocess = submit_button.click(\n",
    "        fn=clear_and_save_textbox,\n",
    "        inputs=textbox,\n",
    "        outputs=[textbox, saved_input],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=display_input,\n",
    "        inputs=[saved_input, chatbot],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            saved_input,\n",
    "            chatbot,\n",
    "            max_new_tokens,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "    )\n",
    "\n",
    "    retry_button.click(\n",
    "        fn=delete_prev_fn,\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, saved_input],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=display_input,\n",
    "        inputs=[saved_input, chatbot],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=generate,\n",
    "        inputs=[\n",
    "            saved_input,\n",
    "            chatbot,\n",
    "            max_new_tokens,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        api_name=False,\n",
    "    )\n",
    "\n",
    "    undo_button.click(\n",
    "        fn=delete_prev_fn,\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, saved_input],\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=lambda x: x,\n",
    "        inputs=[saved_input],\n",
    "        outputs=textbox,\n",
    "        api_name=False,\n",
    "        queue=False,\n",
    "    )\n",
    "\n",
    "    clear_button.click(\n",
    "        fn=lambda: ([], ''),\n",
    "        outputs=[chatbot, saved_input],\n",
    "        queue=False,\n",
    "        api_name=False,\n",
    "    )\n",
    "\n",
    "demo.queue(max_size=20).launch(server_port=8091, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
